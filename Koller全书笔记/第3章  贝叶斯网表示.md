### 第三章  贝叶斯网表示

 	目的是表达随机变量集合$\mathcal{X}=\left\{X_{1}, \cdots, X_{n}\right\}$的联合分布$P$，但是囿于高维分布巨大的计算量和参数需求，要估计出如此庞大的模型参数不现实，因此需要用变量之间的独立性来紧凑表示高维分布。例如对于有若干枚硬币，其中第$i$枚正面朝上的概率为参数$\theta_{i}$，那么对于这些硬币投掷事件的可能情况可以通过联合分布来描述，所需参数为各硬币正面朝上的概率，如所有硬币正面朝上的概率理论上需要参数$2^{n}$-1的条件概率确定，但是由于各硬币之间的条件独立性，可以简化得到结果为$\theta_{1} \cdot \theta_{2} \cdots \cdots \theta_{n}$，只需n个参数。

#### 3.1  独立性性质的利用

##### 3.1.1  随机变量的独立性

描述了边缘独立性用于紧凑联合分布的可能，但是大多数实际应用中边缘独立性是比较少的，需要更一般的形式来紧凑化联合概率分布。

##### 3.1.2  条件参数化方法

本部分由简单的例子导出更一般形式的条件化参数方法。

例子：雇员智力水平和SAT成绩相关性的案例分析。从因果关系来讲，应聘者的SAT成绩是受到智力等多因素决定的，而智力是受遗传、学习等多因素影响。智力和SAT成绩各用一个随机变量描述，其中0表示低水平，1为高水平，可以观测到该例的联合分布表如下：

$\begin{equation}
\begin{array}{cc|c}
{I} & {S} & {P(I, S)} \\
\hline i^{0} & {s^{0}} & {0.665} \\
{i^{0}} & {s^{1}} & {0.035} \\
{i^{1}} & {s^{0}} & {0.06} \\
{i^{1}} & {s^{1}} & {0.24}
\end{array}
\end{equation}$

对上述联合分布，可以用更简洁的方法表示：$P(I, S)=P(I) P(S | I)$，其中智力作为父节点独立提出概率，乘上sat成绩的条件概率得到联合分布结果，利用了智力和sat成绩的依赖关系重新表达分布。这种思路实际上<u>更匹配实际的因果关系</u>（先抛开参数量），<u>尽管在模型中并不严格要求遵循因果关系，但是遵循因果直觉能得到更强的可解释性，一般都会这么做</u>，这里的因果就是智力和SAT以及其他因素的相互影响。

通过上式可以将上表用下面两个表替代：

$\begin{equation}
\begin{array}{c}
 {i_0} & {i_1} \\
\hline  {0.7} & {0.3} 
\end{array}
\end{equation}$

$\begin{equation}
\begin{array}{c|cc}
{I} & {s_0} & {s_1} \\
\hline i^{0} & {0.95} & {0.05} \\
{i^{1}} & {0.2} & {0.8}
\end{array}
\end{equation}$

上表分别表示$I$的先验分布（prior distribution）和$I$给定时$S$的条件概率分布（conditional probability distribution，**CPD**）。上述表述中，使用三个二项分布进行建模，一个是$P(I)$，另外两个是给定$I$的两种情况下的$S$分布，分别为$P\left(S | i^{0}\right)$和$P\left(S | i^{1}\right)$，因此可以用三个参数（三个二项分布的概率）参数化表征该联合分布。【本例中，使用条件参数化比联合分布更自然，但是并没有更紧凑，因为联合分布仍只需要三个参数，就能确定四个概率（和为1）】

##### 3.1.3  朴素贝叶斯模型

###### 3.1.3.1  学生示例

* 案例：在上述雇员案例基础上，除了SAT成绩变量S外，公司还已知学生雇员的在校成绩G，两者综合考量智力水平，其中成绩G分为三个等级，构成共计12个表值的联合分布。

* 分析：（不考虑除设计变量之外的影响因素）
  
  <img src="https://github.com/ming71/PGM/tree/master/pic/1580634389288.png" alt="1580634389288" style="zoom:67%;" />

【注意】这个案例中，首先可知没有任何边缘独立性可言（IG和IS显然不独立，GS显然常识上来说是有关系的也不独立），因此边缘独立性方法失效，尝试通过因果关系建立联系来表达独立性进而简化分布。

（1）首先学生的智力能一定程度决定成绩和SAT分数，可以导出$P(I, S, G)=P(S, G | I) P(I)$，为进一步化简，需要拆分式中的条件概率；
  （2）观察到已知学生智力的情况下（如智商高），那么他的在校成绩高低并不会影响我们对他SAT成绩的判断，所以G和S在给定I的条件下是独立的：$P(S, G | I)=P(S | I) P(G | I)$。【<u>在边缘独立性失效的情况下，我们发现了额外的独立性</u>，事实证明，这种条件独立性表示更自然，解释性更强，而且更具一般性，应用更广泛】。

代入上式得到：$P(I, S, G)=P(S | I) P(G | I) P(I)$；
  （3）由此可以将联合$P(I, S, G)$分解为三个条件概率分布（CPD）的乘积。对于这三个分布$P(S | I) ，P(I)， P(G | I)$前两个在上一部分已经给出，后一个成绩分布如下给出：
  $\begin{equation}
  \begin{array}{c|ccc}
  {I} & {g_1} & {g_2} & {g_3}\\
  \hline i^{0} & {0.2} & {0.34}& {0.46} \\
  {i^{1}} & {0.74} & {0.17}& {0.09}
\end{array}
  \end{equation}$

（4）对于原联合分布需要参数为11（12-1）个，现在的参数化方法更加紧凑只需要：三个二项分布$P(I), \quad P\left(S | i^{0}\right)$，$P\left(S | i^{1}\right)$以及两个三值多项式分布$P\left(G | i^{\mathrm{i}}\right) ， P\left(G | i^{0}\right)$。每个二项分布需要一个参数，每个三值多项式分布需要两个参数，共计7个参数

* 结论：参数更少更加紧凑且自然；**模块性**（对于联合分布而言，新加一个参数成绩G需要重写整个联合分布表，但是对于因子分解表示中，对比上述两例发现只需要多加一个条件分布概率$P(G | I)$，该性质很重要）。

###### 3.1.3.2  一般模型

* **朴素贝叶斯模型**（naive Bayes）：朴素贝叶斯模型假设所有事例属于<u>两两互斥且包含所有事例情况</u>的类（class）中的一个（事例instance属于类class，而类之间两两互斥，可以理解为两两独立）【**正好适合处理分类任务**】，对该类存在一个从某集合$\left\{c^{1}, \cdots, c^{k}\right\}$中取值的类变量$C$（如上例中，类变量是学生智商$I$，类中的两个事例是高智商和低智商的学生）。朴素贝叶斯假设简而言之就是事例的每个类内，不同的观测特征相互独立
* **因子分解**（fatorization）：
朴素贝叶斯模型的条件独立性可以归纳为：对于所有特征$X_{1}, \cdots, X_{k}$，有两两独立性$\left(X_{i} \perp X_{-i} | C\right)$，其中$X_{-i}=\left\{X_{1}, \cdots X_{k}\right\}-\left\{X_{i}\right\}$，该贝叶斯网可以用如下图表示：

<img src="https://github.com/ming71/PGM/tree/master/pic/1581388537935.png" alt="1581388537935" style="zoom:67%;" />

根据上面的学生示例进行一般性推广，依据独立性假设，得到**朴素贝叶斯条件概率模型的因子分解形式**：
$$
P\left(C, X_{1}, \cdots, X_{n}\right)=P(C) \prod_{i=1}^{n} P\left(X_{i} | C\right)
$$
。该式中表示朴素贝叶斯模型可以使用少量的参数（因子）来表达联合概率分布（得益于两两独立的强假设），只需要一个先验分布$P(C)$和一系列条件概率分布$P\left(X_{j} | C\right)$（每个条件概率分布对应一个结果变量）。形式非常自然优雅。
* **==案例分析：朴素贝叶斯分类器解决鸢尾花数据集分类==**

上述描述可知，朴素贝叶斯的参数少模型简单特点很适用于类之间两两互斥的分类任务，早期多用于医疗诊断，现在依然可以用在计算机视觉的分类任务等领域。下面考虑一个基于朴素贝叶斯模型的多分类任务。例如图像分类任务，对一张图像进行分类首先需要提取特征$X_{1}, \cdots, X_{n}$，图像的可能类别集合为$C$={$c_1, c_2,...c_n$}，构成一个父节点为$C$，其决定的子节点特征$X_{1}, \cdots, X_{n}$（同上面的朴素贝叶斯网图）。那么因子分解的概率模型如上面推导所示。可见需要对未知数据进行分类需要知道两个参数：<u>先验分布和一系列条件概率</u>。

数据集介绍：鸢尾花数据集包含150个数据，平均分成三类（Setosa，Versicolour，Virginica），每个数据包含4个属性/特征（花萼长度，花萼宽度，花瓣长度，花瓣宽度）。算法程序如下：

 [NaiveBayes.py](D:\application\visual studio\python\NaiveBayes.py) 

朴素贝叶斯算法是监督学习，使用给定数据的标注来学习模型参数进而做出合理预测。那么他的参数学习体现在哪里？不难看出，就是根据给定数据统计出的<u>先验概率以及各个特征的条件概率</u>，用已有数据和label估计一次性这些参数然后根据参数来判别未知数据的label类别。也不难得出一个重要假设：**独立同分布**（iid）。

#### 3.2  贝叶斯网

 	贝叶斯网建立的直观假设与朴素贝叶斯相同：<u>利用分布的条件独立性来获得紧凑且自然的表示</u>。但贝叶斯网不限制分布必须满足强独立假设更加灵活。贝叶斯网表示的核心是有向无圈图（DAG）$\mathcal{G}$,该图提供了以因子分解方式紧凑表示联合分布骨架的数据结构（通过该DAG可以直观得出独立性假设关系，或是分布的因子分解式）。

##### 3.2.1  学生示例

###### 3.2.1.1  模型建立

* 模型：在之前学生示例基础上进一步复杂化：学生成绩不仅取决于智商，还有课程难度$D$，$\operatorname{Val}(D)=\{\text { easy, hard }\}$；同时，教授为学生写推荐信时只查看学生的在校成绩决定推荐信好坏$L$，$\operatorname{Val}(L)=\{\text { strong, weak }\}$。至此，该学生模型包含五个随机变量$I, D, G, S, L$,其中除了$G$有三个取值外，其他都是两个取值，联合分部总取值为48个。
* 贝叶斯网：贝叶斯网的表示由一系列变量节点和局部概率模型组成，模型的每个变量都关联着一个条件概率分布。有父节点的时父节点为条件指定X的分布，<u>无父节点时空变量集作为条件，条件概率转为边缘概率分布</u>。由网络结构和条件概率分布共同构成贝叶斯网$\mathcal{B}$，本例中用表示$\mathcal{B}^{\text {studem}}$表示学生示例的贝叶斯网。如下图：

<img src="https://github.com/ming71/PGM/tree/master/pic/1580639912471.png" alt="1580639912471" style="zoom:67%;" />
* 概率查询：如果想计算某个事件的概率，可以通过构成它的基本事件概率获得，如$i^{1}, d^{0}, g^{2}, s^{1}, l^{0}$的可能性可以有：$\begin{aligned} P\left(i^{1}, d^{0}, g^{2}, s^{1}, l^{0}\right) &=P\left(i^{1}\right) P\left(d^{0}\right) P\left(g^{2} | i^{1}, d^{0}\right) P\left(s^{1} | i^{1}\right) P\left(l^{0} | g^{2}\right) \\ &=0.3 \cdot 0.6 \cdot 0.08 \cdot 0.8 \cdot 0.4=0.004608 \end{aligned}$
推广到一般情况，对于联合分布任何状态有：
$$
P(I, D, G, S, L)=P(I) P(D) P(G | I, D) P(S | I) P(L | G)
$$
###### 3.2.1.2  推理模式

强调因果关系的相互制约影响。推理是在得到模型情况下，根据已知发生的事件对相关未知事件进行估计。

因果推理（casual reasoning）、预测（prediction）、证据推理（evidential reasoning）、解释（explanation）、解释消除（explaining away）、因果间推理（intercasual reasoning）

##### 3.2.2  贝叶斯网的基本独立性

###### 3.2.2.1  学生示例中的独立性

贝叶斯网语义的核心是<u>“节点只直接依赖其父节点”</u>。在学生示例中，$(L \perp I, D, S | G)$，即一旦知道学生的成绩如何，推荐信好坏只取决于成绩，无关任何其他因素。同理，SAT成绩只取决于智商，而无关其他任何因素，可以表示为$(S \perp D, G, L | I)$，称给定父节点$I$时，$S$与网络中其他节点独立。但不能依样画葫芦说给定$D$，$G$和其他节点独立，这是显然的（很自然可以发现有规律，后面会总结）。类似还有，$(G \perp S | I, D)$，$(I \perp D)$，$(D \perp I, S)$。

<u>一旦给定父节点，那么与其父节点或其他祖先节点直接或间接相关的信息都不会影响该节点的可信度。但是我们可以通过其后代节点的相关信息进行证据推理来改变对该节点的判断。</u>

###### 3.2.2.2  贝叶斯网语义

* 定义：贝叶斯网结构$\mathcal{G}$是其节点代表随机变量$X_{1}, \cdots, X_{n}$的一个DAG。令$\mathrm{Pa}_{X_i}^{\mathcal{G}}$表示$X_{i}$在$\mathcal{G}$中的父节点，$NonDescendants_X$表示$X_{i}$在$\mathcal{G}$中的非后代节点变量，因此$\mathcal{G}$表示了如下称为局部独立性的条件独立假设，记为$\mathcal{I}_{\ell}(\mathcal{G})$：
   	对每个变量$X_{i}$：$\left(X_{i} \perp \text { NonDescendants }_{X_{i}} | \mathrm{Pa}_{X_{i}}^{\mathcal{G}}\right)$
      	即：局部独立性表明，**给定（$X_{i}$的）父节点条件下，每个节点$X_{i}$与其<u>非后代</u>节点条件独立**。
    
* 遗传示例
  血型分为ABO三种，基因和性状上分别由基因型和表现型控制，分别用变量$G(p)$和$B(p)$表示一个人$p$的这两种特征。立即可以获得局部独立性假设，如，一旦知道某个人父母的基因型，那么就能知道这个人所有可能遗传给后代的基因，而他的非后代（祖先）提供的任何信息都改变不了这个事实或者提供新的信息。

#####  3.2.3  图与分布

<u>贝叶斯网图的形式化语义是一系列独立性断言</u>（对独立性的判定），学生贝叶斯网是由条件概率分布注释的图，这个图通过链式法则又定义了一个链式分布。本节将证明这两种定义是等价的。

###### 3.2.3.1  I-Map（independency map）

 	概率图模型就是用简洁明了的 Graph 来紧凑表示复杂的概率分布，Graph 的 node 表示随机变量，edge 表示直接的概率关系。但给定一张 Graph，这张 Graph 能否等价地表示这个概率分布呢？这就是 I-map 这块要回答的。

 	首先定义与分布$P$相关的独立性的集合。
* 定义3.2
  	令$P$是$\mathcal{X}$上的分布。$\mathcal{I}(P)$定义为在$P$中成立的形如$(X \perp Y | Z)$的独立断言的集合。
现可以将陈述“$P$满足与$\mathcal{G}$相关的局部独立性”简写为$\mathcal{I}_{l}(\mathcal{G}) \subseteq \mathcal{I}(P)$，这种情况下，可以称$\mathcal{G}$是$P$的一个**I-map**。
* 定义3.3
  令$\mathcal{K}$是与独立性集合$\mathcal{I}(\mathcal{K})$相关联的任意一个图对象。如果对于独立性集合$\mathcal{I}$，$\mathcal{I}(\mathcal{K}) \subseteq \mathcal{I}$，则称$\mathcal{K}$是一个I-map

简而言之，<u>**I-map是张图**，这个图中包含一些独立性断言，这些独立性断言集是分布$P$独立性断言集合的子集</u>。

* 例3.1分析
  先看定义的图，三个图的节点集都只包含X和Y，区别在于边：$\mathcal{G}_{\varnothing}$无边（隐含独立假设），$\mathcal{G}_{X \rightarrow Y}$和$\mathcal{G}_{Y \rightarrow X}$是指向不同的边,可知，$\mathcal{I_1}(G)=(X \perp Y)$ ,$\mathcal{I_2}(G)=\mathcal{I_3}(G)=\varnothing$；
再看概率分布，不难根据表得知左侧的分布中XY独立，右侧不独立，记为$\mathcal{I_1}(P)=(X \perp Y)$,$\mathcal{I_2}(P)=\varnothing$；
则知，   $\mathcal{I_1}(G),\mathcal{I_2}(G),\mathcal{I_3}(G)\in \mathcal{I_1}(P)$ ，$\mathcal{I_2}(G),\mathcal{I_3}(G)\in \mathcal{I_2}(P)$ ，所以三个图都是左侧$P$的I-Map，图23是右侧$P$的IMap。
  
###### 3.2.3.2  I-map到因子分解

* 贝叶斯网的链式法则：令$\mathcal{G}$为定义在$X_{1}, \cdots, X_{n}$上的一个贝叶斯网，若$P$可以表示为如下乘积：
$$
P\left(X_{1}, \cdots, X_{n}\right)=\prod_{i=1}^{n} P\left(X_{i} | \mathrm{Pa}_{X_{i}}^{\mathcal{G}}\right)
$$
则称分布$P$是关于图$\mathcal{G}$的在同一空间上的**==因子分解==**。该等式称为贝叶斯网的链式法则，单个因子$P\left(X_{i} | \mathrm{Pa}_{X_{i}}^{G}\right)$称为条件概率分布（CPD）或局部概率模型。

* 定理3.1
  令$\mathcal{G}$是定义在变量集$\mathcal{X}$上的一个贝叶斯网结构，并且令$P$是同一空间上的联合分布。如果$P$是$\mathcal{G}$的一个I-map，那么$P$根据$\mathcal{G}$因子分解。
这个定理不难理解，就是分布可以根据图的独立性断言（I-map）进行因子分解。

######  3.2.3.3  因子分解到I-map

前面的定理3.1单向揭示：条件独立性隐含了因子分解。反之仍成立：根据$\mathcal{G}$因子分解隐含了与之相关的条件独立性。

* 定理3.2 
令$\mathcal{G}$是定义在变量集$\mathcal{X}$上的一个贝叶斯网结构，并且令$P$是同一空间上的联合分布。如果$P$根据$\mathcal{G}$因子分解，那么$P$是$\mathcal{G}$的一个I-map。【因子分解和I-map的充要性，条件独立性和因子分解的等价性】
#### 3.3  图中的独立性

###### 3.3.1  d-分离

一个直观例子：

<img src="https://github.com/ming71/PGM/tree/master/pic/1580964826407.png" alt="1580964826407" style="zoom:67%;" />

对于直接连接即图结构中包含$X \rightarrow Y$的情况不予讨论，因为无论$Z$如何，他们之间一定相互依赖。关注$X$和$Y$之间存在一条经过$Z$的迹的情况。

* 间接的因果关系（a）：观察到$Z$的情况，$X$不会通过$Z$作用于$Y$，即$(X \perp Y | Z)$
* 间接的证据作用（b）：和上面的情况对称，$(Y \perp X | Z)$
* 共同的原因（c）：结论而言同样是$(Y \perp X | Z)$，学生示例中$I$是$G$和$S$的父节点，在已知$I$的情况下，后两者并不关联相互独立，都只依赖于$I$
* 共同的结果（d）：没有观察到$Z$时，$X$和$Y$是先验独立的；当$Z$被观测后，反而产生依赖。如学生示例的$I$和$D$，本身是相互独立的，但是如果知道了他们的共同子节点$G$，那么能据此推断出学生的智力水平以及关联到考试难度，例如，知道学生成绩差，那么同时能想到这个学生不聪明，以及考试可能太难了。  

当影响经过$Z$可以从$X$流向$Y$时，称迹$X \rightleftharpoons Z \rightleftharpoons Y$是有效的（active），总结如下：
* 因果迹：$X \rightarrow Z \rightarrow Y$，有效当且仅当没有观察到$Z$
* 证据迹：$X \leftarrow Z \leftarrow Y$，有效当且仅当没有观察到$Z$
* 共同的原因：$X \leftarrow Z \rightarrow Y$，有效当且仅当没有观察到$Z$
* 共同的结果：$X \rightarrow Z \leftarrow Y$，有效当且仅当有<u>观察到$Z$或其一个后代</u>；形如$X \rightarrow Z \leftarrow Y$的结构也称作v-结构。

下面定义有效迹：

* 定义3.6
  令$\mathcal{G}$是一个贝叶斯网结构，且$X_{1} \rightleftharpoons \cdots \rightleftharpoons X_{n}$是$\mathcal{G}$中的一条迹。令$Z$是观测变量的一个子集，在给定$Z$的情况下，假如：
  * 一旦有一个v结构$X_{i-1} \rightarrow X_{i} \leftarrow X_{i+1}$，则$X_{i}$或其一个后代在$Z$中；
  * 迹上其他点都不在$Z$中
那么迹$X_{1} \rightleftharpoons \cdots \rightleftharpoons X_{n}$是**有效迹**（注意：如果$X_{1}$或$X_{n}$在$Z$中，那么迹不是有效的）。可以理解为：有效迹上，影响是可以传递/流动的，相互之间不能独立。

有了有效迹的定义，就可以在更广义的节点集上定义独立性，从而引出下面d-分离以及全局马尔可夫独立性：

* 定义3.7
  令$X, Y, Z$是图$\mathcal{G}$的三个节点集，在给定$Z$的条件下，假如任意节点$X \in \boldsymbol{X}$和$Y \in \boldsymbol{Y}$之间不存在有效迹，那么$X$与$Y$在给定$Z$时是**d-分离**（d-separation）的，记作$\mathrm{d}-\operatorname{sep}_{\mathcal{G}}(\boldsymbol{X} ; \boldsymbol{Y} | \boldsymbol{Z})$。
  与d-分离对应的独立性的集合用$\mathcal{I}(\mathcal{G})$表示：
  $$
  \mathcal{I}(\mathcal{G})=\left\{(X \perp Y | Z): \mathrm{d}-\operatorname{sep}_{\mathcal{G}}(\boldsymbol{X} ; \boldsymbol{Y} | \boldsymbol{Z})\right\}
  $$
该集合称为全局马尔可夫独立性（global Markov independencies）集。
  
###### 3.3.1  可靠性与完备性

* 定理3.3
  如果分布$P$根据$\mathcal{G}$因子分解，那么$\mathcal{I}(\mathcal{G}) \subseteq \mathcal{I}(P)$

###### 3.3.1  d-分离算法
用d-分离的概念可以简单地通过检测$\mathcal{G}$的连通性来推断在$\mathcal{G}$上因子分解的分布$P$的独立性。为使其有用，需要有效确定d-分离。定义的方法只能通过枚举待检测节点集之间的每条迹是否有效，效率低下，计算时间与图的规模呈指数增长。所以需要简便快捷的算法实现。

算法具体很简单，可以查看一下相关部分或者算法文档就行。
参见附件d-separation文件，或者[直译过来的文档](https://zhuanlan.zhihu.com/p/96759877)。

在图中构建d-分离可以直接得到分布P的独立性断言。

##### 3.3.4  I-等价

概念$\mathcal{I}(\mathcal{G})$导出了图的一系列独立关系，那么自然能想到截然不同的贝叶斯网可能有相同的$\mathcal{I}(\mathcal{G})$，即拥有完全相同的独立性断言，那么在某种程度上他们是等价的。比如，之前讨论局部独立性提到的三种结构：$X \rightarrow Z \rightarrow Y$，$X \leftarrow Z \leftarrow Y$和$X \leftarrow Z \rightarrow Y$三者有相同的独立性假设。对于这种等价有如下定义：

* 定义3.9
如果$\mathcal{I}\left(\mathcal{K}_{1}\right)=\mathcal{I}\left(\mathcal{K}_{2}\right)$，那么$\mathcal{X}$上的两个图结构$\mathcal{K}_{1}$和$\mathcal{K}_{2}$是**I-等价**（equivalent）的。

关于I-等价的讨论：
显然，一个分布$P$在一个图上可以因子分解，那么在其I-等价图上也能因子分解，此外$P$中的内在属性一旦与一个图关联，必定能与另一个I-等价图关联，这可以帮助确定影响的方向。

我们可以利用d-分离来检验两个图的I-等价，下面开始讨论：

* 定义3.10 
$\mathcal{X}$上的贝叶斯网$\mathcal{G}$的**骨架**（skeleton）是$\mathcal{X}$上的一个无向图，对于$\mathcal{G}$中的每条边$(X, Y)$，它包含边$\{X, Y\}$

很好理解，骨架，就是直接把方向去掉，那么骨架相同就够了吗？显然不是，如$X \leftarrow Z \rightarrow Y$及其v-结构，显然是个反例。那么骨架相同且v-结构集相同是否就I-等价了呢？很接近了，实际上这个命题只是I-等价的充分条件，还差一点东西。先整理一下这个充分条件：

* 定理3.7 
令$\mathcal{G}_1$和$\mathcal{G}_2$是$\mathcal{X}$上的两个图。如果$\mathcal{G}_1$和$\mathcal{G}_2$有相同的骨架和v-结构集，那么他们是I-等价的。

反例如三个节点构成的完全图（回顾图的定义：可以有单节点的环，但不能有长度为2的环；完全图是再加一条边就形成环的图），三个节点的完全图有三条边8种方向可能性，排除掉成环的两种还剩6种，任取两种：$X \rightarrow Y \rightarrow Z \leftarrow X$， $X \rightarrow Y \leftarrow Z \leftarrow X$，由于两者是完全图，独立性断言必然都是空集，那么两图也就I-等价；但是，由于两者虽然骨架相同，但是v-结构明显不同，是为反例。

为了继续讨论和I-等价完全等价的条件，先给出如下定义：

* 如果$X$与$Y$之间不存在有向边，那么v-结构$X \rightarrow Z \leftarrow Y$是**非正则结构**（immorality）。如果这样的边存在，那么称该边为v-结构的**覆盖边**（covering edge）。
* 定理3.8
令$\mathcal{G}_{1}$，$\mathcal{G}_{2}$是$\mathcal{X}$上的两个图，则$\mathcal{G}_{1}$，$\mathcal{G}_{2}$有相同的骨架和相同的非正则结构集合，当且仅当他们是I-等价的。

####  3.4  从分布到图

从图到分布，通过构建d分离可以从图中得到分布的独立性断言。那么从分布出发，怎样构建一个图，使得这个图中的独立性能够替代分布的所有独立性呢？实际上有一个观点必须**突出**：<u>完全联合分布规模十分庞大，以至于无法对其具体指明，所以实际上没有完全明确的分布$P$,也不会为其建立一个图$G$</u>。

##### 3.4.1  最小I-map

找到表示分布$P$的图一种直观的思路就是先给出他所有的I-map。但是很多I-map我们并不关心（重新回顾一下，I-map是张图，这个图中包含一些独立性断言，这些独立性断言集是分布$P$独立性断言集合的子集），例如完备图，包含的独立性断言为空，无法揭示任何独立性条件，没有意义；此外，很多图中还会有一些冗余的边。所以最小I-map的概念由此衍生：

* 定义3.13
假如$\mathcal{K}$是独立关系集$\mathcal{I}$的一个最小I-map，并且从$\mathcal{K}$中移除一条甚至单边的边都会使其不再是I-map，那么图$\mathcal{K}$是$\mathcal{I}$的最小I-map。
* **寻找最小I-map的算法**
略，需要用到再总结。

##### 3.4.2 P-map

根据之前的定义很容易得到P-map的定义，即一个能恰好描述分布所有独立性断言的图。一般分布可能很难找到甚至没有P-map所以这里就不讨论了。有需要用到再看回来。

---