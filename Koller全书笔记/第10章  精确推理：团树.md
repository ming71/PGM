### 第十章  精确推理：团树

> 写在前面：这部分推荐结合PRML和白板推导，前者是从因子图深入讲解更加直观有助于理解，后者手推原理容易上手。顺序是先白板推导，再PRML最后这本书。
>

**团树（clique tree）算法**也是一种精确推断算法，又称**联合树**（junction tree）或**连接树**。之前的VE算法存在的问题是，每次进行概率查询时，都需要重新进行和积运算，中间变量没有保留，导致大量的重复运算效率低下。而团树结构改进了这一点，仍采用变量消除的思想，但计算每一个中间信息传播的作用，从而将整个概率查询化为这些信息传播的组合。具体方法是利用原模型构造一个团树结构，在团之间进行消息传递并校准团树从而获得所求概率

#### 10.1  变量消除与团树

首先明确一点，有向图可以道德化表示为无向图，从分布函数上看无向图也是有向图更一般化的表示，所以下面都是针对无向图的讨论，有向图转化同理。

##### 10.1.1  聚类图

定义而言：聚类图是这样一个无向图：节点是团$C_i\subseteq \mathcal{X}$，其中节点$C_i$和$C_j$之间的边是表子集$\boldsymbol{S}_{i, j} \subseteq \boldsymbol{C}_{i} \cap \boldsymbol{C}_{j}$。
【**<u>解读聚类图</u>**】：聚类图有两个要素：节点和边。其中，聚类图的节点是原图的一个子集（往往是团），节点的初始位势就是这些原图的因子初始化的（如各因子条件概率的乘积）；边是相邻节点的信息交流路径，对应着两个节点共有的变量。


聚类图的特点及理解：
* 聚类图是无向图
* 执行一次变量消除定义一个聚类图 （每次变量消除会使若干因子抱团——节点，并进行信息传播——边）
* 消除$\psi_{i}$（分布的条件概率）中一个变量产生的信息$\tau_{i}$（消除变量后产生的新条件概率）用于计算$\tau_{j}$（要求的边缘或者条件概率），那么在两个聚类$C_i$和$C_j$之间画一条边
* 聚类图无环（避免正反馈，否则信息传播不能收敛）
* 原图每个势函数都被使用，且被使用一次
* 每个消息如果存在两个知道的人，这两个人必须要有交流途径

直接看一个例子：回到之前的学生贝叶斯网如下，变量消除顺序为$C, D,I,H,G,S,L$，得到的聚类图连接起来如图。例如，执行第一次消除变量$C$，产生的条件概率新因子是$\phi_{c}(d)$，即消去了$C$，和$D$相关，从而得到第一个聚类无向图，后面依次如此。具体的消息传播和因子变化如下。

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1581741218524.png" alt="1581741218524" style="zoom: 67%;" />

<img src="https://images2015.cnblogs.com/blog/710098/201601/710098-20160122162524984-459114869.png" alt="img" style="zoom: 67%;" />


##### 10.1.2  团树

团树是一种特殊的聚类图，在团树中，这些聚类直接称之为团。团树满足聚类图的两个属性：

1. Family Preservation：原图每个因子都与一个聚类图的团对应
2. Running Intersection Property：对于每一对包含了同一变量$X$的团$C_i$、$C_j$应该存在且只存在一条包含变量$X$的路径。

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1581853683704.png" alt="1581853683704" style="zoom:50%;" />

#### 10.2  消息传递：和积

前面是从变量消除的角度理解团树，现在从团树描述变量消除的执行过程。值得关注的是，团树结构的边为消息存储、缓存计算提供了便利性，从而可以通过计算信念传播巧妙解决VE存在的重复低效计算的问题，以空间换时间。下面在图模型中用团树执行精确推理。

##### 10.2.1  团树中的变量消除

###### 10.2.1.1  一个例子（重要）

该例子是学生分布的一个部分，引出了一些后面常用的表达方式以及定义，值得仔细看一下。

* 初始位势：通过为团指定的初始因子相乘产生的。有向图而言可以是条件概率，无向图直接是能量函数。注意这种赋值方法可能不止一个。进一步理解可以从因子图的角度去看。
* 就绪的（ready）：一个团已经完成对其所有传入消息的收集，那么称该团是就绪的。

###### 10.2.1.2  团树消息传递

下面将上例中的表示方法进行总结归纳。

每个因子$\phi \in \mathbf{\Phi}$被指定给一些团$\alpha(\phi)$,那么 $C_{i}$的初始势能定义为：
$$
\psi_{j}\left(\boldsymbol{C}_{j}\right)=\prod_{\phi: \alpha(\phi)=j} \phi
$$
由于每个因子都恰好指定给一个团，所以有：
$$
\prod_{\phi} \phi=\prod_{j} \psi_{j}
$$
令$C_r$为选出的根团，现在该团上执行和-积变量消除，从团树的叶节点向上移动。对每个$C_i$，$Nb_i$定义为$C_i$的近邻团的索引集。从团$C_i$到另一个团$C_j$的消息为：
$$
\delta_{i \rightarrow j}=\sum_{C_{i}-S_{i, j}} \psi_{i} \cdot \prod_{k \in\left(\mathrm{Nb}_{i}-\{j\}\right)} \delta_{k \rightarrow i}
$$
当根团接收到所有消息后，把这些消息和自己的初始势能相乘，得到一个因子称为置信或信念（belief），记为$\beta_{r}\left(\boldsymbol{C}_{r}\right)$：
$$
\tilde{P}_{\Phi}\left(C_{r}\right)=\sum_{\chi-C_{r}} \prod_{\phi} \phi
$$

<u>【注意】其他非根团的传递也有类似的信念定义，并不局限特定根团</u>，只是上面讨论的是根团所以才这么说。

【**从MRF的因子分解理解变量消除**】

参照下图，首先写出马尔可夫网的因子分解形式，根据这些因子来定义团的因子。例如计算b到a传播的信息，

1. $m_{c \rightarrow b}$$m_{d \rightarrow b}$，是b的所有非a子节点信息传给b的信息相乘（联合概率就是乘积的形式），每个乘子是一个节点传递出的相关信息；
2. 然后将这个全部信息在b进行处理——乘以势函数$\psi(b)$，得到的结果$\psi(b)m_{c \rightarrow b}m_{d \rightarrow b}$是b节点能够传出的用b表示的全部信息，也就是**信念belief(b)**；
3. 传给a时需要乘上他们之间的边位势代表这些b发出的所有信息有多少会发送给a，然后求和把b积分边缘化掉得到真正到达a的信息$\delta_{b \rightarrow a}$；
4. 在根节点a进行同样的乘以势函数的操作代表接受的信息，考虑到a实际不止b一个子节点，因此计算$P(a)$时还有对a所有的邻居节点进行同样的操作而加个连乘。

不断进行上述递归操作就能计算出需求的边缘概率。【从变量可以推而广之到团就能很好理解信息流动了】

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1581846101566.png" alt="1581846101566" style="zoom:50%;" />

附上电子科大的一个总结，PPT见[这里](http://dm.uestc.edu.cn/wp-content/uploads/seminar/GraphModel_9_10_Inference.pdf)：

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1581860441214.png" alt="1581860441214" style="zoom: 50%;" />

###### 10.2.1.3  正确性

#####  10.2.2  团树校准

为什么要引入团树的校准？1. 为了能够利用置信计算边缘分布，必须保证团树上的信息传播是收敛的  2. 相邻团之间关于割集的置信应该确保一致（例如两个人经过一番讨论，最后关于明天是否下雨的可能性结论应该能达成共识）。

* 定义10.5
如果$\sum_{C_{i}-S_{i, j}} \beta_{i}\left(\boldsymbol{C}_{i}\right)=\sum_{C_{j}-S_{i, j}} \beta_{j}\left(\boldsymbol{C}_{j}\right)$，那么两个相邻的团$C_i$和$C_j$称为是**校准的**（calibrated）。
如果所有成对的相邻团都是校准的，那么称树$\mathcal{T}$是校准的，对于校准树，用$\beta_{i}\left(\boldsymbol{C}_{i}\right)$表示**团置信**（ clique beliefs）；并定义**割集置信**（sepset beliefs）：
$$
\mu_{i, j}\left(\boldsymbol{S}_{i, j}\right)=\sum_{C_{i}-S_{i, j}} \beta_{i}\left(\boldsymbol{C}_{i}\right)=\sum_{\boldsymbol{C}_{j}-\boldsymbol{S}_{i, j}} \beta_{j}\left(\boldsymbol{C}_{j}\right)
$$
割集置信就是将置信度按照非交集/割集变量进行<u>边缘化</u>，获得边缘概率分布便于信息交流。另外，<u>校准的即代表了团树是收敛的</u>。

可以进一步推理得到：
$$
\begin{aligned}
\mu_{i, j}\left(\boldsymbol{S}_{i, j}\right) &=\sum_{C_{i}-S_{i, j}} \beta_{i}\left(\boldsymbol{C}_{i}\right) \\
&=\sum_{C_{i}-S_{i, j}} \psi_{i} \cdot \prod_{k \in \mathrm{Nb}_{i}} \delta_{k \rightarrow i} \\
&=\sum_{C_{i}-\boldsymbol{S}_{i, j}} \psi_{i} \cdot \delta_{j \rightarrow i} \prod_{k \in\left(\mathrm{Nb}_{i}-\{j\}\right)} \delta_{k \rightarrow i} \\
&=\delta_{j \rightarrow i} \sum_{C_{i}-\boldsymbol{S}_{i, j}} \psi_{i} \cdot \prod_{k \in\left(\mathrm{Nb}_{i}-\{j\}\right)} \delta_{k \rightarrow i} \\
&=\delta_{j \rightarrow i} \delta_{i \rightarrow j}
\end{aligned}
$$

由上面推导过程和结果可以得到团树校准和收敛的等价性证明，相关内容这里不赘述，参考Koller的公开课

##### 10.2.3  将校准团树作为一个分布

* 命题10.3
在团树校正算法的收敛处，下式成立：
$$
\tilde{P}_{\Phi}(\mathcal{X})=\frac{\prod_{i \in \mathcal{V}_{\mathcal{T}}} \beta_{i}\left(\boldsymbol{C}_{i}\right)}{\prod_{(i-j) \in \mathcal{E}_{\mathcal{T}}} \mu_{i, j}\left(\boldsymbol{S}_{i, j}\right)}
$$
式中，$\tilde{P}_{\phi}(\mathcal{X})$是所有节点变量未归一化的联合分布，右边是用所有团置信除以了所有割集置信，可见该式将联合分布和团树连接起来，具体形式见证明的推导。团置信和割集置信为非归一化度量提供了一个**再参数化**(reparameterization)，该性质称为**团树不变量**(clique tree invariant)。即再参数化可以不用能量函数$\psi$，而是用基于信念消息进行联合分布的定义。

由这个命题可以导出正式定义由团树表示的分布：

* 定义10.6
由校准的团树$\mathcal{T}$诱导的度量定义为：
$$
Q_{\mathcal{T}}=\frac{\prod_{i \in \mathcal{V}_{\mathcal{T}}} \beta_{i}\left(\boldsymbol{C}_{i}\right)}{\prod_{(i-j) \in \mathcal{E}_{\mathcal{T}}} \mu_{i, j}\left(\boldsymbol{S}_{i, j}\right)}
$$
其中，$\mu_{i, j}=\sum_{C_{i}-S_{i, j}} \beta_{i}\left(\boldsymbol{C}_{i}\right)=\sum_{C_{j}-S_{i, j}} \beta_{j}\left(\boldsymbol{C}_{j}\right)$。

【**再参数化理解**】怎么理解？看下面图说话！懂啦：和MRF因子分解是一样的，结果上等价于原始分布！也就原始联合分布的另一种形式，但是参数不再是势函数因子而是置信。

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582381375688.png" alt="1582381375688" style="zoom: 33%;" />

#### 10.3  消息传递：置信更新

##### 10.3.1  使用除法的消息传递

介绍了和之前等价的一种Lauritzen-Spiegelhalter算法，也叫置信-更新消息传递。是一种并行算法，不需要等待节点的就绪。对应参考看算法10.3，下面理解一下：

【算法解读】首先初始化团树，然后执行迭代：选择一对节点，执行更新算法：

1. 初始化：每个节点的初始置信就是自身势函数
2. 更新置信：在一次更新过程中，先将发送节点的置信边缘化，存储更新到割集置信中，然后接收节点的置信更新为乘以一个比例，该比例由当前传递信息除以原来的割集置信所得
3. 【例10.8】

【关于传播收敛性的补充】

置信传播是否一定收敛？不一定。怎样从数学上验证？很难。有哪些结论？参见koller的斯坦福公开课视频BP in practice节。并行算法速度快，但是收敛慢。这种并行迭代的更新算法本质是一种近似方法。

##### 10.3.3  回答查询

#### 10.4  构建一个树

构建团树有两种方法：基于变量消除；基于有向图运算。

##### 10.4.1  源自变量消除的团树

在变量消除中一次变量消除所涉及到的所有变量构成一个团，去掉被消除的变量剩下的变量成为指向下一个团的有向边。如果一个团的所有变量被包含在了另一个团中，则去除这个团。

##### 10.4.2  源自弦图的团树

参见定理4.12。

---