### 第八章  指数族

#### 8.1  引言

本章采用白板推导的思路进行，同时对书中的部分加以对比解释。

#### 8.2  指数族

之前考虑的都是特定的单个分布表示，下面讨论享有相同参数形式、只有特定参数选择不同的分布构成的**分布族**（families of distributions ）。

指数分布族在上世纪30年代中期被提出，在概率论和统计学中，它是一些有着特殊形式的概率分布的集合，包括许多常用的分布，如<u>正态分布、指数分布、伯努利分布、泊松分布、gamma分布、beta分布</u>等等。指数分布族为很多重要而常用的概率分布提供了统一框架，这种一般性有助于表达的方便和从更大的宏观尺度上理解这些分布。

首先给出**==指数分布族的标准形式==**：
$$
P(x | \eta)=h(x) \exp \left(\eta^{\top} \phi(x)-A(\eta)\right)
$$
固定其他参数的情况下，随着$\eta$的变化，我们可以在这个指数族中得到得到关于参数$\eta$的不同分布。

* 定义8.1
  回到书本上，书上的定义更为详尽和严谨，定义为：
  $$
  P_{\theta}(\xi)=\frac{1}{Z(\theta)} A(\xi) \exp \{\langle\mathrm{t}(\theta), \tau(\xi)\rangle\}
  $$
其中$\langle\mathbf{t}(\boldsymbol{\theta}), \tau(\xi)\rangle$是向量内积，配分函数有：
$$
Z(\boldsymbol{\theta})=\sum_{\xi} A(\xi) \exp \{\langle\mathbf{t}(\boldsymbol{\theta}), \tau(\xi)\rangle\}
$$
对数配分函数和上面定义对比**区别和进一步理解解读：

1. 把归一化因子拿到exp外面了，对数配分函数变成了配分函数；
2. 指数项上的内积和$\eta^{T}(\theta) \cdot \phi(x)$相对应，一个是自然参数函数、一个是充分统计量；
3. 充分统计量这里表示为变量集$\mathcal{X}$到一个k维空间$\mathcal{R}^{K}$的函数$\tau$，高斯分布为例，原分布的统计量为一维的，但是指数族分布的描述需要二维空间$k=2$；
4. 合法参数$\theta$（原像，原分布的参数）的凸集构成一个参数空间$\Theta \subseteq \mathcal{R}^{M}$，$\theta \in \Theta$，注意维度是M，仍以高斯分布为例，单变量正态分布参数空间为二元的，而指数族分布上不一定就是二维的，可能有三维参数（虽然实际上推导发现还是二维）；
5. 自然参数函数是将原像空间$R^{M}$映射到像空间$R^{K}$（两个空间的维度可以一致）
6. $Z(\theta)$就是直接的配分函数（和马尔可夫网一致）；$\mathcal{P}=\left\{P_{o}: \boldsymbol{\theta} \in \Theta\right\}$体现的就是随参数变化能得到指数族中不同的分布。

很多常见分布都可以殊途同归地化为指数分布族的一个特例，不同之处就是四个参数的选择，下面举几个例子：

* 伯努利分布化为指数分布族形式
一次伯努利试验满足二项分布：$P(y ; \varphi)=\varphi^{y}(1-\varphi)^{1-y} \quad y \in 0,1$，则可由此：
$$
\begin{aligned}
\mathrm{P}(\mathrm{y} ; \varphi) &=\varphi^{y}(1-\varphi)^{1-y}=\exp \left(\ln \varphi^{y}(1-\varphi)^{1-y}\right) \\
&=\exp (\mathrm{yln} \varphi+(1-\mathrm{y}) \ln (1-\varphi)) \\
&=\exp \left(\mathrm{yln} \frac{\varphi}{1-\varphi}+\ln (1-\varphi)\right)
\end{aligned}
$$
从而得到指数分布族形式，其中参数为：
$$
h(y)=1，\\
\phi(y)=y，\\
\eta=\ln \frac{\varphi}{1-\varphi}，\Rightarrow \varphi=\frac{1}{1+e^{-\eta}}\\
A(\eta)=-\ln (1-\varphi)=\ln \left(1+e^{\eta}\right)
$$
* 高斯分布化为指数分布族形式
给定高斯分布：$P(x | \theta)=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right\} \quad \theta=\left(\mu, \sigma^{2}\right)$，可见参数由两个统计量组成，由此：
$$
\begin{align*}
P(x | \theta)&=\frac{1}{\sqrt{2 \pi} \sigma} \exp \left\{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}\right\} \quad, \theta=\left(\mu, \sigma^{2}\right)\\
&=\frac{1}{\sqrt{2 \pi \sigma^{2}}} \exp \left\{-\frac{1}{2 \sigma^{2}}\left(x^{2}-2 \mu x+\mu^{2}\right)\right\}\\
&=\exp \log \left(2 \pi \sigma^{2}\right)^{-\frac{1}{2}} \exp \left\{-\frac{1}{2\sigma^{2}} \underbrace{\left(x^{2}-2 \mu x\right)}-\frac{\mu^{2}}{2\sigma^{2}}\right\}\\
&=\exp \log \left(2 \pi \sigma^{2}\right)^{-\frac{1}{2}} \cdot \exp \left\{-\frac{1}{2\sigma^{2}}(-2 \mu  \quad 1)\left(\begin{array}{l}
{x} \\
{x^{2}}
\end{array}\right)-\frac{\mu^{2}}{2 \sigma^{2}}\right\}\\
&=\exp \{\underbrace{\left(\frac{\mu}{\sigma^{2}} \quad  -\frac{1}{2\sigma^{2}}\right)}_{\eta^{T}}  \underbrace{\left(\begin{array}{l}
{x} \\
{x^{2}}
\end{array}\right)}_{\phi^{x}}\underbrace{\left.-\left(\frac{\mu^{2}}{2\sigma^{2}}+\frac{1}{2} \log 2 \pi \sigma^{2}\right)\right\}}_{A(\eta)}

\end{align*}
$$
得到该形式后和标准指数分布族对比：
$$
\eta=\left(\begin{array}{l}
{\eta_{1}} \\
{\eta_{2}}
\end{array}\right)
=\left(\begin{array}{c}
{\frac{\mu}{\sigma^{2}}} \\
{-\frac{1}{2\sigma^{2}}}
\end{array}\right)
\Rightarrow \left\{\begin{array}{l}
{\mu=-\frac{\eta_{1}}{2 \eta_{2}}} \\
{\sigma^{2}=-\frac{1}{2 \eta_{2}}}
\end{array}\right.
$$
$$
 \phi(x)=\left(\begin{array}{c}
{x} \\
{x^{2}}
\end{array}\right)
$$
$$
\begin{aligned}
A(\eta) &=-\frac{\eta_{1}^{2}}{4 \eta_{2}}+\frac{1}{2} \log \left(2 \pi-\frac{1}{2 \eta_{2}}\right) \\
&=-\frac{\eta_{1}^{2}}{4 \eta_{2}}+\frac{1}{2} \log \left(-\frac{\pi}{\eta_{2}}\right)
\end{aligned}
$$

此外还可以试着化一下多项式、泊松分布等（例8.6就是多项式分布的结果）。

##### 8.2.1  线性指数族

线性指数族是指数分布族的子族，包括： 正态分布、二项分布、泊松分布、负二项分布、gamma分布，逆高斯分布等，其特例是自然参数函数为恒等函数（维度不变），更一般而言，线性指数族的特点是<u>参数维度在自然参数函数的映射前后保持不变</u>，也就是定义形式归纳的不同点4中，原分布和指数族分布的参数维度一样，可以简化形式为：
$$
P_{\theta}(\xi)=\frac{1}{Z(\boldsymbol{\theta})} \exp \{\langle\boldsymbol{\theta}, \tau(\xi)\rangle\}
$$
区别在于把自然参数函数的符号去掉了，其中$\theta$是原参数空间的一个备选（注意是参数空间，不是原分布的参数；二者只是维度一致）。
* 例8.4

虽然可能按照线性指数族的映射方式来选择参数，但显然并不一定都能成功，下面说明这一点。高斯分布为例，我们推到得到的形式是线性的，那么这个参数是不是随便取都能成立（即高斯分布都能化成指数族形式）呢？有没有什么约束？

有。<u>约束是配分函数概率和为1</u>。为了使得配分函数能够满足概率和为1，现在进行验证，按照之前推导取一阶矩和二阶矩作为充分统计量，在未知参数具体值情况下，按照能化为线性指数族的假设（参数维度不变，所以可表示为$\eta=(\eta_1 \ \eta_2)$），进行直观论证：
$$
\begin{aligned}
Z(\boldsymbol{\eta}) &=\int \exp \{\langle\boldsymbol{\eta}, \tau(x)\rangle\} d x =\int_{-\infty}^{\infty} \exp \left\{\eta_{1} x+\eta_{2} x^{2}\right\} d x
\end{aligned}
$$
该表示中，括号内是一个过原点的二次函数，如果开口向上$\eta_{2} \geqslant 0$，则在x趋向正负无穷方向上，二次函数为正无穷，加上指数底后积分是无限的；只有开口向下时，无穷处二次函数趋向于负无穷，加上指数底后被积对象趋近0，可知这个区间积分是有限的（面积有限），只有这样才能归一化概率。检查上面的结果知，之前推导的$\eta_{2} $由于$\sigma^2>0$，则其必为负，所以这个配分函数是有效的。因此正态分布是线性指数族的。

根据上述讨论，必然存在一些合理的自然参数集合，这些参数构成**自然参数空间**（参数维度在映射前后两个空间是一致的）。定义就是能够得到有限积分结果的配分函数，从而使得化出的线性指数族表示有意义：
$$
\Theta=\left\{\boldsymbol{\theta} \in \mathcal{R}^{K}: \int \exp \{\langle\boldsymbol{\theta}, \tau(\xi)\rangle\} \mathrm{d} \xi<\infty\right\}
$$
总的来说，**线性指数族**满足两个条件：（1）自然参数函数的等维度映射，且<u>自然参数空间是开凸</u>的 （2）线性指数族形式的有效性（配分函数是定积分）。

* 例8.5
虽然之前已经证明过，其实就是伯努利的指数族化。但是从线性指数族视角来看，提供了不一样的信息。他先用例8.2给出的参数进行验证，显然这个是非凸的。但是可行，而且维度也ok，说明他是自然参数空间的子空间，那么需要找到更大的空间。后面得到比较合适的结果就是之前我们自己推导的参数$t(\theta)=\ln \frac{\theta}{1-\theta}$。


#### 8.3  因式化的指数族(factored exponential families)

上面讨论的都是单变量的情况，下面讨论多元分布。其实在推导马尔可夫网的因子分解定义的对数线性模型已经可以看出两者是等价的了。

##### 8.3.1  乘积分布

指数族因子中，对于一个非归一化因子：$\phi_{\theta}(\xi)=A(\xi) \exp \{\langle\operatorname{t}(\theta), \tau(\xi)\rangle\}$，多个因子合成的乘积分布为：$P_{\theta}(\xi) \propto \prod_{i} \phi_{\theta_{i}}(\xi)=\left(\prod_{i} A_{i}(\xi)\right) \exp \left\{\sum_{i}\left\langle t_{i}\left(\theta_{i}\right), \tau_{i}(\xi)\right\rangle\right\}$。

指数族因子的乘积也属于指数族。

##### 8.3.2  贝叶斯网

由上面可知，有指数形式的CPD贝叶斯网也定义了一个指数族。（注意每个因子都必须是归一化的，否则没有配分函数不能等价成指数分布族形式）

结论而言，（1）有指数形式CPD乘积的一个贝叶斯网定义了一个指数族，（2）但为了获得贝叶斯的指数表示，必须保证每个CPD都是局部归一化的。


#### 8.4  熵和相对熵

##### 8.4.1  熵

分布的**熵**（entropy）是对分布中“随机性”或“噪声”的测量，也称作**信息熵**。熵低意味着分布的大部分集中在少量实例中；熵高意味着更接近均匀分布（最大熵显然是均匀分布）。另一种解释是，熵是分布中编码一个实例平均所需的比特数。

首先看有关**==熵的定义==**。**==信息量==**：概率的负对数$-\log p(x)$；熵是信息量关于分布的期望：
$$
\begin{aligned} H_p(x)=E_{p(x)}[-\log p(x)] &=\int-p(x) \cdot \log p(x) d x \\ &=-\sum_{x} p(x) \cdot \log p(x) \end{aligned}
$$

###### 8.4.1.1  指数模型的熵

实际上从最大熵原理可以推出满足指数分布族。

* 定理8.1
令$P_{\theta}$为指数族的一个分布，则其熵：
$$
H_{P_{\theta}}(\mathcal{X})=\ln Z(\theta)-\left\langle E_{P_{\theta}}[\tau(\mathcal{X})], \mathrm{t}(\theta)\right\rangle
$$
怎么得到的呢？首先看之前的指数分布族定义：$P_{\theta}(\xi)=\frac{1}{Z(\theta)} A(\xi) \exp \{\langle\mathrm{t}(\theta), \tau(\xi)\rangle\}$，可以得到该概率分布的信息量，即在右边取一个负对数（这里底是$e$，实际上还有取2和10的，含义不同都是信息熵），然后根据熵的定义，在右边再求一个期望就行。

例8.11的高斯分布求熵有个取巧的办法：不用定理8.1+指数族形式来求，直接根据熵的定义，取负对数然后信息量的期望来求会更简单。

* 命题8.1 
如果$P(\mathcal{X})=\frac{1}{Z} \prod_{k} \phi_{k}\left(D_{k}\right)$是一个马尔可夫网，其熵：
$$
\boldsymbol{H}_{p}(\boldsymbol{x})=\ln Z+\sum_{k} \boldsymbol{E}_{p}\left[-\ln \phi_{k}\left(\boldsymbol{D}_{k}\right)\right]
$$

###### 8.4.1.2  贝叶斯网的熵

【补充】**条件熵**：
$$
\begin{aligned}
H(Y | X) &=\sum_{x \in \mathcal{X}} p(x) H(Y | X=x) \\
&=-\sum_{x \in \mathcal{X}} p(x) \sum_{x \in \mathcal{Y}} p(y | x) \log p(y | x) \\
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(y | x) \\
&=-E \log p(Y | X)
\end{aligned}
$$

* 定理8.2
如果$P(\mathcal{X})=\prod_{i} P\left(X_{i} | \mathrm{Pa}_{i}^{\mathcal{G}}\right)$是与贝叶斯网$\mathcal{G}$一致的一个分布，那么：
$$
\boldsymbol{H}_{p}(\mathcal{X})=\sum_{i} \boldsymbol{H}_{p}\left(X_{i} | \mathrm{Pa}_{i}^{\mathcal{G}}\right)
$$
证明见书上，就是把因子乘积形式和条件熵的结果结合起来。结论而言，似乎看到贝叶斯网的熵被分解成了单个条件分布的条件熵之和，也就是说贝叶斯网的熵可以直接由CPD决定。实际上并不如此。

* 命题8.2
如果$P(\mathcal{X})=\prod_{i} P\left(X_{i} | \mathrm{Pa}_{i}^{\mathcal{G}}\right)$是与贝叶斯网$\mathcal{G}$一致的一个分布，那么：

$$
\sum_{i} \min _{\mathbf{p a}_{i}^{\mathcal{G}}} \boldsymbol{H}_{P}\left(X_{i} | \mathrm{pa}_{i}^{\mathcal{G}}\right) \leq \boldsymbol{H}_{P}(\mathcal{X}) \leq \sum_{i} \max _{\mathbf{p a}_{i}^{\mathcal{G}}} \boldsymbol{H}_{P}\left(X_{i} | \mathrm{pa}_{i}^{\mathcal{G}}\right)
$$

##### 8.4.2  相对熵

**==相对熵==**（relative entropy）是一种**量化两种概率分布P和Q之间差异**的方式，又称作==**K-L散度**==。例如给定一系列样本数据，现有两种分布来描述该样本特征，哪一个能更多地保留或拟合原数据呢？可以用相对熵来衡量。一个很好的例子：==[如何理解信息熵](https://www.jianshu.com/p/43318a3dc715)==。

下面给出K-L散度的计算公式。设$p$为观察得到的概率分布，$q$为另一分布来近似$p$，则$p$、$q$的K-L散度为:
$$
D_{K L}(p \| q)=\sum_{i=1}^{N} p\left(x_{i}\right) \cdot\left(\log p\left(x_{i}\right)-\log q\left(x_{i}\right)\right)
$$
或表示为：
$$
D_{K L}(p \| q)=\sum_{i=1}^{N} p\left(x_{i}\right) \cdot \log \frac{p\left(x_{i}\right)}{q\left(x_{i}\right)}
$$
需要注意的是：<u>两个分布pq不是对等</u>的，即$\mathrm{KL}(p \| q) \not \equiv \mathrm{KL}(q \| p)$，所以书中将K-L散度当做距离度量实际有问题，因为距离度量应该满足对称性（维基百科定义）。当$p=q$时，两个分布一致，相对熵为0。

从上面定义不难看出，<u>K-L散度其实是真实分布对数据的原始分布p和近似分布q之间的对数差值的期望</u>，所以也可以写作：
$$
D_{K L}(p \| q)=E[\log p(x)-\log q(x)]
$$
进一步，在定义的基础上再作变形：
$$
\begin{aligned}
&D_{K L}(p \| q)=E_{p}\left[\log \frac{p(x)}{q(x)}\right]=\sum_{x \in \mathcal{X}} p(x) \log \frac{p(x)}{q(x)}\\
&=\sum_{x \in \mathcal{X}}[p(x) \log p(x)-p(x) \log q(x)]\\
&=\sum_{x \in \mathcal{X}} p(x) \log p(x)-\sum_{x \in \mathcal{X}} p(x) \log q(x)\\
&=-H(p)-\sum_{x \in \mathcal{X}} p(x) \log q(x)\\
&=-H(p)+E_{p}[-\log q(x)]\\
&=H_{p}(q)-H(p)
\end{aligned}
$$
这种形式（实际没必要这么复杂的化简，从定义就能看出来）中，当底数取2时，后一项是信息熵，表示用$p$进行编码需要的比特数；前一项是在分布$p$下，使用$q$进行编码需要的比特数（因为$H_{p}(q)$的中信息量定义项是$- \log q(x)$）。所以另一种相对熵的理解是：<u>$D_{K L}(p \| q)$表示在真实分布为$p$的前提下，使用$q$分布进行编码相对于使用真实分布$p$进行编码（即最优编码）所多出来的bit数。这个值越大，说明编码需要的空间越大，差异越大</u>
其中，我们再看一个更常用的概念：==**交叉熵**==（cross entropy）。交叉熵也是用于度量两个分布的差异，定义如下：
$$
H(p, q)=-\sum_{i=1}^{n} p\left(x_{i}\right) \log \left(q\left(x_{i}\right)\right)=H_{p}(q)
$$
显然交叉熵也是不对成的，按照相对熵的理解，不难认为，<u>交叉熵$H(p, q)$是使用$q$分布编码分布$p$需要的bit数</u>。不难得出，他和相对熵之间的关系为：**K-L散度=交叉熵-熵**（1. 交叉熵反而没有熵 2. 注意是谁的熵，谁对谁的交叉熵，顺序不能反）：
$$
C E H(p, q)=E_{p}[-\log q]=-\sum_{x \in \mathcal{X}} p(x) \log q(x)=H(p)+D_{K L}(p \| q)
$$
==**案例推导：交叉熵解决二分类问题**==

1. 任务：在已有的数据上训练一个分类模型，对给定的图片进行分类，判断其是否有物体
  
  2. 分析：物体的有无从预测和真实分布上是两个单独的0-1分布；分布参数需要模型最后能够给出一个合法的概率，可以用sigmoid函数实现；优化方法选择梯度下降，则需要一个loss来记录偏离gt的程度，下面讨论这一项。
  
  3. 分类建模：例如当前判断的是类别是猫，那么要照片中是否是猫，真实照片为猫记作标签1，反之为0。真实样本分布记为$p$，服从参数为$p$的0-1分布，即$X∼B(1,p)$ ；预测样本分布记为$q$，服从参数为$q$的0-1分布，即$X∼B(1,q)$ ，需要$q$通过学习能够拟合真实分布$p$，$q$的概率由sigmoid给出。
  
  4. 单个样本的交叉熵计算：
  
     （注意顺序：是用预测分布来编码真实分布，时刻注意CE的不对称性）

$$
\begin{array}{l}
  {C E H(p, q)} \\
  {=-\sum_{x \in \mathcal{X}} \mathbf{p}(\mathbf{x}) \log \mathbf{q}(\mathbf{x})} \\
  {=-\left[P_{p}(x=1) \log P_{q}(x=1)+P_{p}(x=0) \log P_{q}(x=0)\right]} \\
  {=-[p \log q+(1-p) \log (1-q)]} \\
  
  \end{array}
$$

  	5. 分析：$p=1$时，$loss=-log \ q$，负对数函数，于$q=1$时匹配真实分布，惩罚为0；$q<1$时根据sigmoid输出偏离真实分布情况来惩罚 。

  	6. inference：训练好模型后，inference时直接输出，根据sigmoid结果设置一个阈值如0.5，大于为正样本，小于为负样本。

* ==**案例推导：交叉熵应用于多分类问题***==
1. 任务：同样是图像分类，但是此时图像中可能不止一种物体，需要分类时一次给出该图像属于多个类别各自的概率。
2. 建模：分析和上面相同，但是模型输出的最后一层不再使用单变量的sigmoid而是多变量softmax：$y_{i}=\frac{e^{a_{i}}}{\sum_{k=1}^{C} e^{a_{k}}}$，其中$C$为类别数，即通过指数运算将目标属于每个类别的参数归一化，这样一来属于不同类别的概率之间是互斥的，概率和为1。其他部分相同
3. 数值爆炸：实际上由于网络输出不稳定，一旦指数值过大，很容易出现nan导致训练停止，所以往往会想要将指数拉到0附近。处理方法是选取最大值因子$F=-\max \left(a_{1}, a_{2}, \dots, a_{C}\right)$，然后分子分母同时除去该值$\frac{e^{a_{i} + F}}{\sum_{k=1}^{C} e^{a_{k}+F}}$
4. inference：选取所有类别中最高的一个为判定的类别。
5. 与二分类sigmoid的异同：
   1. softmax在类别为2的情况下等价于sigmoid
   2. softmax要求被分类别两两互斥，例如猫狗人，对于有重叠的标签无法准确分类，如猫，狗，动物，车辆这种
   3. 可以用sigmoid+one-hot构建多个二分类器解决交叉标签的问题

扯远了，回到书本上来，给出相对熵定义：
* 定理8.3
  在由$\tau, \mathrm{t}$定义的指数族中考虑分布$Q$和分布$P_{\theta}$则：

$$
\boldsymbol{D}\left(Q \| P_{\theta}\right)=-\boldsymbol{H}_{Q}(\mathcal{X})-\left\langle\boldsymbol{E}_{Q}[\tau(\mathcal{X})], \mathbf{t}(\boldsymbol{\theta})\right\rangle+\ln Z(\boldsymbol{\theta})
$$
很好解读，根据书中给的熵形式套一下直接能看出，第一项是真实分布的熵，第二项是预测分布对真实分布的编码交叉熵。



#### 8.5  投影

上面定义的相对熵为衡量两个分布之间的差距提供了有效手段。考虑一个问题：在给定的指数族内，寻找关于相对熵与给定分布最接近的一个分布。这个问题的应用场景有：（1）用简单结构近似复杂分布 （2）网络的近似推理。

假定有一个分布$P$并希望用分布类$\mathcal{Q}$（如指数族）中的一个分布$Q$来近似他。注意到由于K-L散度是不对称的，因此这种衡量有两种形式：

* 定义8.4
  令$P$是一个分布，并且令$\mathcal{Q}$是分布的凸集。
  * I-投影（information projection）：$P$到$\mathcal{Q}$的I-投影是分布：$Q^{I}=\arg \min _{Q \in \mathcal{Q}} \boldsymbol{D}(Q \| P)$
  * M-投影（moment projection）：$P$到$\mathcal{Q}$的M-投影是分布：$Q^{M}=\arg \min _{Q \in \mathcal{Q}} \boldsymbol{D}(P \| Q)$

##### 8.5.1  比较

和线面投影类比就很好理解了$P$和$Q$是两条线，其中$Q$在面$\mathcal{Q}$上，就不难理解投影的不对称性，以及子集的特殊性了。注意两个投影都是最相似分布，类比到线段就是作面的垂直投影线才能保证最似（此时投影最短）。此外还有如：熵不对称类比于两条不共面线段的内积不同；如果共面，那么最佳投影一定是和其一模一样的线段，所以一样。

【解读】前面知道K-L散度是不同编码方式需要的额外空间，那么投影可以视为寻找对应编码方式下最小信息损失。（最小化额外编码空间，而额外编码意味着熵增即信息损失）。后面在进行变分推断时，优化目标会等效到K-L散度表示的投影上。

* 例8.13
有一个非高斯的分布$P$，现在在高斯分布族$\mathcal{Q}$上寻找一个分布$Q$使得两者的差异最小，分别用两种投影能找到两个分布如下图

<u>M-投影试图给每个赋值给予合理的高概率，而I-投影则试图把分布$P$中具有高概率的赋值作为重点，并保持合理的熵</u>。因而M-投影相对而言有更大的方差，I-投影更为集中。

##### 8.5.2  M-投影

* 命题8.3
令$P$是$X_{1}, \cdots, X_{n}$上的一个分布，且$\mathcal{Q}$是与空图$\mathcal{G}_{\phi}$一致的分布。那么$Q^{M}=\arg \min _{Q=\mathcal{G}_{\emptyset}} \boldsymbol{D}(P \| Q)$是分布：
$$
Q^{M}\left(X_{1}, \ldots, X_{n}\right)=P\left(X_{1}\right) P\left(X_{2}\right) \cdots P\left(X_{n}\right)
$$
即：$P$的因子分解的分布上的M-投影是$P$的边缘分布的简单乘积。

##### 8.5.3  I-投影

---