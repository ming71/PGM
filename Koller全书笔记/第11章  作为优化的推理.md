### 第十一章  作为优化的推理

> 本章重点是变分推断，可参照PRML阅读，**白板推导手推公式必看**。

精确推理算法的局限性在于其计算复杂度过高，因此很多情况需要近似推理。本章首先将精确推理过渡到优化问题，然后再考虑近似推理。

#### 11.1  从推理到优化

* 变分法做了一件什么事？将精确推理问题转化为一个优化问题，从而一来可以用优化方法来解决，二来可以从优化的角度对其作出解释，进而赋予精确推理两层意义：1.消息传递的角度 2.优化问题的描述。

* 怎么做？推理难点在于最大后验和边缘的求解，那么可以找到一个比较简单的分布$q$来近似已知分布$p$，这个$q$形式简单求取容易。怎么度量两个分布的距离？答案在指数族分布中讲过：K-L散度。从而通过衡量K-L散度求取分布$q$，将推理问题转化为一个优化问题。
* 这个优化问题具体是什么？下面展开讨论：

首先看衡量指标K-L散度，直觉上采用M-投影比较易于理解，但是其仍包含难以求取的原分布边缘概率，所以<u>选取I-投影</u>。再者，即使直接优化I-投影$\arg \min _{\varrho} \boldsymbol{D}\left(P_{\Phi} \| Q\right)$也不简单，所以需要找到进一步的解决方案：

【**从对数似然手推变分优化：白板系列**】从边缘分布的概率出发，第一步引入贝叶斯公式，第二步是通过恒等变换引入分布$q$，得到证据下界（ELBO），由于等式左边无关分布$q$，所以右边最小化相对熵等价于最大化$\mathcal{L}(q)$，这个就是书中提到的一个变分：能量泛函。

#### 11.2  作为优化的精确推理

白板推导使用两种方法导出变分推断的解决方案：坐标上升和随机梯度上升法。而这里采用的是一组不动点方程描述，通过对优化问题使用拉格朗日乘子法求取驻点得到局部极值，从而获取不动点迭代方程。

#### 11.3  基于传播的近似

##### 11.3.1  一个引例

这里简单介绍了聚类图中的置信传播，之前再团树中的很多算法在这里都能继续适用，但是依然有区别。比如之前提到团树不能有环，这里考虑了环置信传播的例子，发现信息传播虽然迭代后趋向收敛，但是已经偏离了真实后验。

所以结论而言，我们仍可以采用之前的置信传播方法，因为他是局部模型依然成立，但是不能保证一定收敛到很好的结果。专栏中提到环置信传播重新被启用来自turbocodes的良好尝试，介绍如下图，其中两个解码器之间互相用对方的后验构成循环就是环置信传播的应用。效果非常好，所以 被重视和尝试，但是没有算法基础依然不知道其收敛性如何。

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582467217096.png" alt="1582467217096" style="zoom:50%;" />

##### 11.3.2  聚类图置信传播

和团树的区别：

1. 割集不等价与两个节点共有的变量
2. 聚类图的校准只需要关于割集有一致的边缘分布即可，不用对所有共有变量

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582468554737.png" alt="1582468554737" style="zoom:67%;" />

* 算法11.1
（理解不一定对）在团树中我们置信BP是从叶节点开始，然后根据就绪传播原则计算消息；但对于如上图的多环聚类图，每个节点都不止一条路径，也就是没有任何团在初始化时是就绪的，似乎被锁死了。解决方法是初始化时，令所又的消息初始为1，这样等价于将割集置信设为1，和之前的L-S算法一致，然后开始迭代即可，无需就绪了。

##### 11.3.3  聚类图置信传播的性质

###### 11.3.3.1  重参数化

略，和团树的再参数化推导过程是一样的，即用团置信和割集置信定义原分布。这里提一下流动相交性，和执行相交性都是防止某一个变量在环中无限传播。（有环没关系，可能能收敛，但是一个变量一直传播就不行了）

###### 11.3.3.2  树的一致性

**树的一致性**：$\beta_{i}\left(\boldsymbol{C}_{i}\right)=P_{T}\left(\boldsymbol{C}_{i}\right)$。其中$T$是从原聚类图中导出的一棵树，树的一致性讲的是计算多环聚类图置信时可以通过删除（固定）其他变量来导出一棵树，在这棵树上求置信，结果等价于原聚类图置信。

举的例子说明了：在多环聚类图中边缘分布不能简单通过置信来直接获得。例如之前的ABCD构成的环，有四个团，删去AD的团得到树，在树上可以计算树上的边缘分布$\beta_{1}(A, B)=P_{T}(A, B)$，但是由于原聚类图定义的分布是包含这个被删去的团的，所以这棵树上求的边缘分布不等于整张图的边缘分布结果：$\beta_{1}(A, B) \neq P_{\phi}(A, B)$。

##### 11.3.4  收敛性分析

##### 11.3.5  构建聚类图

######  11.3.5.1  成对马尔可夫网

很多问题可以自然表示成马尔可夫网，这个就不赘述了，之前为了简单直观理解团树和聚类图，就是用MRF作为引例介绍的，只是由于MRF可能成环，所以采用loopy BP。

###### 11.3.5.2  Bethe聚类图

形式非常直观，如下图a：

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582471766854.png" alt="1582471766854" style="zoom: 50%;" />



###### 11.3.5.3  边缘概率之外

上面的标准Bethe聚类图没有刻画变量之间的相互作用，很简单的一种思路是再构建一层如上图b，这样的操作会使得计算代价增大很多；那么降低参数量，直接全部都刻画成对变量之间的影响，舍弃单变量的节点如下图a，但是会不满足流动相交性，适当修改使之满足即可，如图b：

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582471973031.png" alt="1582471973031" style="zoom:50%;" />

* **专栏11.B：使环状置信传播实践中可行的技巧**
1. 非收敛性是局部问题，网络大部分是收敛的，在特定时刻早停计算置信；
2. 非收敛性往往是震荡引起的，通过引入阻尼减少两个更新之间的差异可以抑制这种震荡：$\delta_{i \rightarrow j} \leftarrow \lambda\left(\sum_{C_{i}-S_{i j}} \psi_{i} \prod_{k \neq j} \delta_{k \rightarrow i}\right)+(1-\lambda) \delta_{i-j}^{\mathrm{old}}$。形式上看和momentum思想类似，在更新参数时保留一部分原来的参数进行加权，避免更新过大导致震荡；
3. 同步更新算法：结果而言速度最快，但是往往收敛性不好；
4. **树重新参数化**(tree reparameterization, TRP)：在多环聚类图中构造出一系列包含所有边的树，然后遍历这些树逐个校准信息达到收敛。如下图所示，先固定其他参数，选蓝色树校准，然后选红色树依次进行，只要保证能够覆盖聚类图所有的边就行。这种方法在异步调度中是最快的收敛性也很好；

<img src="C:\Users\xiaoming\AppData\Roaming\Typora\typora-user-images\1582512178264.png" alt="1582512178264" style="zoom: 50%;" />

5. 剩余置信传播（residual belief propagation）：动态地检测出网络中哪些部分是最有用的，相关可以参考koller发表的论文；
6. 通用的方法：直接优化能量泛函（ELBO），这种思路有多种方法，基本的一种是附录A5.2的梯度上升法,最大化ELBO是一个随机梯度上升优化SGVI。这些方法虽然有效但是复杂，所以应用相对不广。

##### 11.3.6  变分分析

##### 11.3.7  其他熵近似

##### 11.3.8  讨论

虽然无法确保网络收敛，但是我们能得到一些似乎与之相关结论：

* 网络拓扑：包含大量短环路的网络更可能不收敛。另外，已经证明聚类图置信传播对于单环是能够收敛的。
* 多大程度上参数化网络的因子是偏斜（分布）的，或说是接近于确定性的。

相对于采样方法的基本确定能收敛，聚类图置信传播的精确推理反而有时候赋予解过高的自信，甚至有时候是错的，然而也没有方法证明何时由聚类图置信传播得到的结果是合理的。

#### 11.4  近似消息传播

11.1-11.3是从特例引入消息近似、消息传递算法，从而在11.4导出更一般的期望传播，这里时间缘故直接跳到期望传播。

##### 11.4.4  期望传播

这里写的比较少，而且形式比较抽象，建议参考PRML的证明和具体算法推导。或者看[这个推导](https://msgsxj.cn/2018/10/02/%E6%9C%9F%E6%9C%9B%E4%BC%A0%E6%92%AD/)和[这个介绍](https://remonstrate.wordpress.com/tag/probabilistic-graphical-model/page/3/)。

思路上，从之前变分推断优化I-投影的对立面出发，直接优化M-投影，并且令分布$q$是在指数族中，那么经过推理可以发现关于自然参数函数的驻点处，最优解仅仅进行了充分统计量期望的匹配，即期望信息可以传递过去，那么在进行分布匹配是只用估计期望就能达到最小K-L散度，故又称**矩匹配**。

例如用一个高斯族的$q$近似一个分布，则计算充分统计量为一阶矩和二阶矩的期望即可，而这两个矩直接定义了高斯分布。可以参见[这个推导](https://msgsxj.cn/2018/10/02/%E6%9C%9F%E6%9C%9B%E4%BC%A0%E6%92%AD/)，以及PRML部分的具体算法（这个才是最终的落脚点）。

EP算法不保证收敛，但是当假设q是指数族分布形式时，如果最后结果收敛，那么最终的解一定是一个驻点。而VI算法通过不断迭代来最大化似然函数的下界，它的每次迭代都是能保证下界是增加的。

#### 11.5  结构化的变分近似

正如之前白板推导中基于平均场假设推导的变分推断迭代公式，在得到ELBO进行优化时，使用了平均场假设，对待优化的因子形式分布$q$假定不同因子之间完全独立，从而推出迭代公式。类似这种假设中，包含了一定的结构信息使得推理易于处理。更一般地说，我们可以通过选择分布$q$的形式来选择任何图模型的结构，通过选择变量之间相互作用的多少来灵活地决定近似程度的大小。 这种完全通用的图模型方法被称为结构化变分推断 。

##### 11.5.1  平均场近似

其实方法和之前很类似，仍是平均场假设上的推理，通过拉格朗日乘子的选择来满足约束$q$的约束空间方程，在选择的乘子上能最大化ELBO。然后层层推导得到了坐标上升法的迭代公式。

---